{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "from keras.applications import MobileNet\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/alvaroviebrantz/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3368: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "mobile = keras.applications.mobilenet.MobileNet()\n",
    "def prepare_image(file):\n",
    "    img_path = ''\n",
    "    img = image.load_img(img_path + file, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array_expanded_dims = np.expand_dims(img_array, axis=0)\n",
    "    return keras.applications.mobilenet.preprocess_input(img_array_expanded_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('n02124075', 'Egyptian_cat', 0.5395711),\n",
       "  ('n02123597', 'Siamese_cat', 0.24800476),\n",
       "  ('n02127052', 'lynx', 0.1419059),\n",
       "  ('n02110185', 'Siberian_husky', 0.0059532803),\n",
       "  ('n04589890', 'window_screen', 0.004288093)]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_image = prepare_image('output/popcorn/2.jpg')\n",
    "predictions = mobile.predict(preprocessed_image)\n",
    "results = imagenet_utils.decode_predictions(predictions)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras_applications/mobilenet.py:208: UserWarning: MobileNet shape is undefined. Weights for input shape (224, 224) will be loaded.\n",
      "  warnings.warn('MobileNet shape is undefined.'\n"
     ]
    }
   ],
   "source": [
    "base_model=MobileNet(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
    "\n",
    "x=base_model.output\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "x=Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "x=Dense(1024,activation='relu')(x) #dense layer 2\n",
    "x=Dense(512,activation='relu')(x) #dense layer 3\n",
    "preds=Dense(5,activation='softmax')(x) #final layer with softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(inputs=base_model.input,outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_2\n",
      "1 conv1_pad\n",
      "2 conv1\n",
      "3 conv1_bn\n",
      "4 conv1_relu\n",
      "5 conv_dw_1\n",
      "6 conv_dw_1_bn\n",
      "7 conv_dw_1_relu\n",
      "8 conv_pw_1\n",
      "9 conv_pw_1_bn\n",
      "10 conv_pw_1_relu\n",
      "11 conv_pad_2\n",
      "12 conv_dw_2\n",
      "13 conv_dw_2_bn\n",
      "14 conv_dw_2_relu\n",
      "15 conv_pw_2\n",
      "16 conv_pw_2_bn\n",
      "17 conv_pw_2_relu\n",
      "18 conv_dw_3\n",
      "19 conv_dw_3_bn\n",
      "20 conv_dw_3_relu\n",
      "21 conv_pw_3\n",
      "22 conv_pw_3_bn\n",
      "23 conv_pw_3_relu\n",
      "24 conv_pad_4\n",
      "25 conv_dw_4\n",
      "26 conv_dw_4_bn\n",
      "27 conv_dw_4_relu\n",
      "28 conv_pw_4\n",
      "29 conv_pw_4_bn\n",
      "30 conv_pw_4_relu\n",
      "31 conv_dw_5\n",
      "32 conv_dw_5_bn\n",
      "33 conv_dw_5_relu\n",
      "34 conv_pw_5\n",
      "35 conv_pw_5_bn\n",
      "36 conv_pw_5_relu\n",
      "37 conv_pad_6\n",
      "38 conv_dw_6\n",
      "39 conv_dw_6_bn\n",
      "40 conv_dw_6_relu\n",
      "41 conv_pw_6\n",
      "42 conv_pw_6_bn\n",
      "43 conv_pw_6_relu\n",
      "44 conv_dw_7\n",
      "45 conv_dw_7_bn\n",
      "46 conv_dw_7_relu\n",
      "47 conv_pw_7\n",
      "48 conv_pw_7_bn\n",
      "49 conv_pw_7_relu\n",
      "50 conv_dw_8\n",
      "51 conv_dw_8_bn\n",
      "52 conv_dw_8_relu\n",
      "53 conv_pw_8\n",
      "54 conv_pw_8_bn\n",
      "55 conv_pw_8_relu\n",
      "56 conv_dw_9\n",
      "57 conv_dw_9_bn\n",
      "58 conv_dw_9_relu\n",
      "59 conv_pw_9\n",
      "60 conv_pw_9_bn\n",
      "61 conv_pw_9_relu\n",
      "62 conv_dw_10\n",
      "63 conv_dw_10_bn\n",
      "64 conv_dw_10_relu\n",
      "65 conv_pw_10\n",
      "66 conv_pw_10_bn\n",
      "67 conv_pw_10_relu\n",
      "68 conv_dw_11\n",
      "69 conv_dw_11_bn\n",
      "70 conv_dw_11_relu\n",
      "71 conv_pw_11\n",
      "72 conv_pw_11_bn\n",
      "73 conv_pw_11_relu\n",
      "74 conv_pad_12\n",
      "75 conv_dw_12\n",
      "76 conv_dw_12_bn\n",
      "77 conv_dw_12_relu\n",
      "78 conv_pw_12\n",
      "79 conv_pw_12_bn\n",
      "80 conv_pw_12_relu\n",
      "81 conv_dw_13\n",
      "82 conv_dw_13_bn\n",
      "83 conv_dw_13_relu\n",
      "84 conv_pw_13\n",
      "85 conv_pw_13_bn\n",
      "86 conv_pw_13_relu\n",
      "87 global_average_pooling2d_2\n",
      "88 dense_1\n",
      "89 dense_2\n",
      "90 dense_3\n",
      "91 dense_4\n"
     ]
    }
   ],
   "source": [
    "for i,layer in enumerate(model.layers):\n",
    "  print(i,layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable=False\n",
    "# or if we want to set the first 20 layers of the network to be non-trainable\n",
    "for layer in model.layers[:20]:\n",
    "    layer.trainable=False\n",
    "for layer in model.layers[20:]:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 148 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
    "\n",
    "train_generator=train_datagen.flow_from_directory('output',\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# Adam optimizer\n",
    "# loss function will be categorical cross entropy\n",
    "# evaluation metric will be accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.2214 - acc: 0.9609\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.1001 - acc: 0.9681\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.1118 - acc: 0.9841\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.1937 - acc: 0.9644\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.1497 - acc: 0.9688\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0105 - acc: 1.0000\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.1282 - acc: 0.9766\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0118 - acc: 1.0000\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0089 - acc: 1.0000\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0506 - acc: 0.9922\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0058 - acc: 1.0000\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0156 - acc: 0.9920\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.3102 - acc: 0.9569\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0865 - acc: 0.9922\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0531 - acc: 0.9681\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.2647 - acc: 0.9287\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0372 - acc: 0.9841\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.1195 - acc: 0.9686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13f0eacc0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "model.fit_generator(generator=train_generator,\n",
    "                   steps_per_epoch=step_size_train,\n",
    "                   epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, show=False):\n",
    "\n",
    "    img = image.load_img(img_path, target_size=(150, 150))\n",
    "    img_tensor = image.img_to_array(img)                    # (height, width, channels)\n",
    "    img_tensor = np.expand_dims(img_tensor, axis=0)         # (1, height, width, channels), add a dimension because the model expects this shape: (batch_size, height, width, channels)\n",
    "    img_tensor /= 255.                                      # imshow expects values in the range [0, 1]\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(img_tensor[0])                           \n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(pred):\n",
    "    predicted_class_indices = np.argmax(pred,axis=1)\n",
    "    labels = (train_generator.class_indices)\n",
    "    labels = dict((v,k) for k,v in labels.items())\n",
    "    predictions = [(labels[k], pred[0][k]) for k in predicted_class_indices] \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/raspberry/8.jpg [('raspberry', 1.0)]\n",
      "output/raspberry/9.jpg [('raspberry', 0.9999962)]\n",
      "output/raspberry/14.jpg [('muffin', 0.9582715)]\n",
      "output/raspberry/15.jpg [('muffin', 1.0)]\n",
      "output/raspberry/17.jpg [('raspberry', 0.99997723)]\n",
      "output/raspberry/16.jpg [('muffin', 0.99396014)]\n",
      "output/raspberry/12.jpg [('muffin', 1.0)]\n",
      "output/raspberry/13.jpg [('muffin', 0.9999993)]\n",
      "output/raspberry/11.jpg [('muffin', 0.86153007)]\n",
      "output/raspberry/10.jpg [('raspberry', 0.999948)]\n",
      "output/raspberry/21.jpg [('raspberry', 0.93273777)]\n",
      "output/raspberry/20.jpg [('raspberry', 0.9988482)]\n",
      "output/raspberry/22.jpg [('muffin', 0.93410486)]\n",
      "output/raspberry/23.jpg [('muffin', 1.0)]\n",
      "output/raspberry/26.jpg [('raspberry', 0.99999404)]\n",
      "output/raspberry/18.jpg [('muffin', 1.0)]\n",
      "output/raspberry/24.jpg [('muffin', 0.990168)]\n",
      "output/raspberry/25.jpg [('muffin', 0.9998136)]\n",
      "output/raspberry/19.jpg [('muffin', 1.0)]\n",
      "output/raspberry/4.jpg [('raspberry', 0.84297764)]\n",
      "output/raspberry/5.jpg [('raspberry', 0.9498381)]\n",
      "output/raspberry/7.jpg [('raspberry', 1.0)]\n",
      "output/raspberry/6.jpg [('raspberry', 0.97640103)]\n",
      "output/raspberry/2.jpg [('raspberry', 0.9995384)]\n",
      "output/raspberry/3.jpg [('raspberry', 0.9999931)]\n",
      "output/raspberry/1.jpg [('muffin', 0.99999416)]\n",
      "output/popcorn/8.jpg [('muffin', 0.9961901)]\n",
      "output/popcorn/9.jpg [('muffin', 0.99429756)]\n",
      "output/popcorn/14.jpg [('muffin', 1.0)]\n",
      "output/popcorn/28.jpg [('muffin', 1.0)]\n",
      "output/popcorn/15.jpg [('muffin', 0.99981886)]\n",
      "output/popcorn/17.jpg [('muffin', 0.99999106)]\n",
      "output/popcorn/16.jpg [('muffin', 0.80688274)]\n",
      "output/popcorn/12.jpg [('popcorn', 0.99319685)]\n",
      "output/popcorn/13.jpg [('popcorn', 0.9993291)]\n",
      "output/popcorn/11.jpg [('muffin', 0.984945)]\n",
      "output/popcorn/10.jpg [('muffin', 1.0)]\n",
      "output/popcorn/21.jpg [('muffin', 0.9058379)]\n",
      "output/popcorn/20.jpg [('popcorn', 0.99995863)]\n",
      "output/popcorn/22.jpg [('popcorn', 0.9999162)]\n",
      "output/popcorn/23.jpg [('popcorn', 0.9999863)]\n",
      "output/popcorn/27.jpg [('muffin', 0.99998736)]\n",
      "output/popcorn/26.jpg [('popcorn', 0.99961853)]\n",
      "output/popcorn/18.jpg [('popcorn', 0.99999917)]\n",
      "output/popcorn/24.jpg [('muffin', 0.99999976)]\n",
      "output/popcorn/25.jpg [('muffin', 1.0)]\n",
      "output/popcorn/19.jpg [('muffin', 1.0)]\n",
      "output/popcorn/4.jpg [('muffin', 1.0)]\n",
      "output/popcorn/5.jpg [('muffin', 0.93859434)]\n",
      "output/popcorn/7.jpg [('muffin', 0.999997)]\n",
      "output/popcorn/6.jpg [('popcorn', 0.937845)]\n",
      "output/popcorn/2.jpg [('muffin', 1.0)]\n",
      "output/popcorn/3.jpg [('popcorn', 0.7521893)]\n",
      "output/popcorn/1.jpg [('muffin', 0.99874425)]\n",
      "output/muffin/8.jpg [('muffin', 1.0)]\n",
      "output/muffin/9.jpg [('muffin', 1.0)]\n",
      "output/muffin/14.jpg [('muffin', 1.0)]\n",
      "output/muffin/15.jpg [('muffin', 1.0)]\n",
      "output/muffin/17.jpg [('muffin', 1.0)]\n",
      "output/muffin/16.jpg [('muffin', 1.0)]\n",
      "output/muffin/12.jpg [('muffin', 1.0)]\n",
      "output/muffin/13.jpg [('muffin', 1.0)]\n",
      "output/muffin/11.jpg [('muffin', 1.0)]\n",
      "output/muffin/10.jpg [('muffin', 1.0)]\n",
      "output/muffin/21.jpg [('muffin', 1.0)]\n",
      "output/muffin/20.jpg [('muffin', 1.0)]\n",
      "output/muffin/22.jpg [('muffin', 1.0)]\n",
      "output/muffin/23.jpg [('muffin', 1.0)]\n",
      "output/muffin/18.jpg [('muffin', 1.0)]\n",
      "output/muffin/24.jpg [('muffin', 1.0)]\n",
      "output/muffin/25.jpg [('muffin', 1.0)]\n",
      "output/muffin/19.jpg [('muffin', 1.0)]\n",
      "output/muffin/4.jpg [('muffin', 1.0)]\n",
      "output/muffin/5.jpg [('muffin', 1.0)]\n",
      "output/muffin/7.jpg [('muffin', 1.0)]\n",
      "output/muffin/6.jpg [('muffin', 1.0)]\n",
      "output/muffin/2.jpg [('muffin', 1.0)]\n",
      "output/muffin/3.jpg [('muffin', 1.0)]\n",
      "output/muffin/1.jpg [('muffin', 1.0)]\n",
      "output/berry/8.jpg [('muffin', 1.0)]\n",
      "output/berry/9.jpg [('berry', 0.9999993)]\n",
      "output/berry/14.jpg [('muffin', 1.0)]\n",
      "output/berry/28.jpg [('berry', 1.0)]\n",
      "output/berry/29.jpg [('berry', 0.99958307)]\n",
      "output/berry/15.jpg [('berry', 0.9999399)]\n",
      "output/berry/17.jpg [('muffin', 1.0)]\n",
      "output/berry/16.jpg [('berry', 1.0)]\n",
      "output/berry/12.jpg [('berry', 0.9246477)]\n",
      "output/berry/13.jpg [('muffin', 1.0)]\n",
      "output/berry/39.jpg [('berry', 0.983185)]\n",
      "output/berry/11.jpg [('muffin', 0.99994576)]\n",
      "output/berry/10.jpg [('berry', 1.0)]\n",
      "output/berry/38.jpg [('muffin', 1.0)]\n",
      "output/berry/21.jpg [('muffin', 1.0)]\n",
      "output/berry/35.jpg [('berry', 1.0)]\n",
      "output/berry/34.jpg [('muffin', 1.0)]\n",
      "output/berry/20.jpg [('muffin', 1.0)]\n",
      "output/berry/36.jpg [('berry', 1.0)]\n",
      "output/berry/22.jpg [('muffin', 1.0)]\n",
      "output/berry/23.jpg [('berry', 1.0)]\n",
      "output/berry/37.jpg [('muffin', 0.979231)]\n",
      "output/berry/33.jpg [('muffin', 1.0)]\n",
      "output/berry/27.jpg [('berry', 1.0)]\n",
      "output/berry/26.jpg [('muffin', 0.44367567)]\n",
      "output/berry/32.jpg [('berry', 0.9995096)]\n",
      "output/berry/18.jpg [('muffin', 0.9999995)]\n",
      "output/berry/24.jpg [('muffin', 0.9983897)]\n",
      "output/berry/30.jpg [('muffin', 1.0)]\n",
      "output/berry/31.jpg [('muffin', 1.0)]\n",
      "output/berry/25.jpg [('muffin', 1.0)]\n",
      "output/berry/19.jpg [('berry', 0.9999999)]\n",
      "output/berry/4.jpg [('berry', 0.99999964)]\n",
      "output/berry/5.jpg [('muffin', 0.99456304)]\n",
      "output/berry/7.jpg [('berry', 1.0)]\n",
      "output/berry/6.jpg [('muffin', 0.9999989)]\n",
      "output/berry/2.jpg [('muffin', 0.9996495)]\n",
      "output/berry/3.jpg [('berry', 0.9999999)]\n",
      "output/berry/1.jpg [('berry', 1.0)]\n",
      "output/jam/8.jpg [('muffin', 0.99988496)]\n",
      "output/jam/9.jpg [('jam', 0.99870825)]\n",
      "output/jam/14.jpg [('muffin', 0.99995005)]\n",
      "output/jam/28.jpg [('muffin', 0.64330506)]\n",
      "output/jam/29.jpg [('popcorn', 0.9997539)]\n",
      "output/jam/15.jpg [('popcorn', 0.9918609)]\n",
      "output/jam/17.jpg [('muffin', 0.9999672)]\n",
      "output/jam/16.jpg [('jam', 0.7520374)]\n",
      "output/jam/12.jpg [('muffin', 1.0)]\n",
      "output/jam/13.jpg [('muffin', 0.9998753)]\n",
      "output/jam/11.jpg [('jam', 0.9999112)]\n",
      "output/jam/10.jpg [('jam', 0.99995553)]\n",
      "output/jam/21.jpg [('jam', 0.99993396)]\n",
      "output/jam/20.jpg [('muffin', 0.64626706)]\n",
      "output/jam/22.jpg [('jam', 0.99999106)]\n",
      "output/jam/23.jpg [('muffin', 0.9998841)]\n",
      "output/jam/27.jpg [('muffin', 0.98367727)]\n",
      "output/jam/26.jpg [('muffin', 1.0)]\n",
      "output/jam/18.jpg [('muffin', 0.9999813)]\n",
      "output/jam/24.jpg [('popcorn', 0.9997769)]\n",
      "output/jam/30.jpg [('muffin', 1.0)]\n",
      "output/jam/25.jpg [('jam', 0.9682226)]\n",
      "output/jam/19.jpg [('jam', 0.9999182)]\n",
      "output/jam/4.jpg [('muffin', 0.9742255)]\n",
      "output/jam/5.jpg [('jam', 0.9762248)]\n",
      "output/jam/7.jpg [('jam', 0.9999995)]\n",
      "output/jam/6.jpg [('muffin', 0.9999988)]\n",
      "output/jam/2.jpg [('muffin', 0.98735434)]\n",
      "output/jam/3.jpg [('popcorn', 0.9996573)]\n",
      "output/jam/1.jpg [('jam', 0.7841749)]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "#img_path = 'output/jam/10.jpg'\n",
    "paths = glob.glob(\"output/**/*.jpg\")\n",
    "\n",
    "for img_path in paths:\n",
    "    new_image = load_image(img_path)\n",
    "    pred = model.predict(new_image)\n",
    "    predictions = decode_predictions(pred)\n",
    "    print(img_path, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/popcorn_test.png [('muffin', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "img_path = \"test/popcorn_test.png\"\n",
    "new_image = load_image(img_path)\n",
    "pred = model.predict(new_image)\n",
    "predictions = decode_predictions(pred)\n",
    "print(img_path, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model/keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        # Graph -> GraphDef ProtoBuf\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = convert_variables_to_constants(session, input_graph_def,\n",
    "                                                      output_names, freeze_var_names)\n",
    "        return frozen_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-106178dfd08b>:31: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 714 variables.\n",
      "INFO:tensorflow:Converted 714 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "frozen_graph = freeze_session(K.get_session(),\n",
    "                              output_names=[out.op.name for out in model.outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.write_graph(frozen_graph, \"model\", \"cats_model.pb\", as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'dense_4/Softmax:0' shape=(?, 5) dtype=float32>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'input_2:0' shape=(?, ?, ?, 3) dtype=float32>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'berry': 0, 'jam': 1, 'muffin': 2, 'popcorn': 3, 'raspberry': 4}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MODEL_NAME = 'model'\n",
    "PATH_TO_CKPT = MODEL_NAME + '/cats_model.pb'\n",
    "PATH_TO_LABELS = os.path.join(MODEL_NAME, 'graph.pbtxt')\n",
    "NUM_CLASSES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_graph = tf.Graph()\n",
    "with classification_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "    classification_sess = tf.Session(graph=classification_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "cats_label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "cats_categories = label_map_util.convert_label_map_to_categories(\n",
    "    cats_label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "cats_category_index = label_map_util.create_category_index(cats_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(image_np_expanded, sess, graph):\n",
    "    #image_np_expanded = np.expand_dims(frame, axis=0)\n",
    "    image_tensor = graph.get_tensor_by_name('input_4:0')\n",
    "    classes = graph.get_tensor_by_name('dense_12/Softmax:0')\n",
    "    (classes) = sess.run(\n",
    "        [classes],\n",
    "        feed_dict={image_tensor: image_np_expanded})\n",
    "    local_classes = np.squeeze(classes)\n",
    "    min_score_thresh = .5\n",
    "    print('classify image, found ', local_classes)\n",
    "    for i in range(len(local_classes)):\n",
    "        score = local_classes[i]\n",
    "        # if score > min_score_thresh:\n",
    "        class_name = cats_category_index[i+1]['name']\n",
    "        print('found ', i, class_name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify image, found  [7.1741763e-18 8.9007530e-27 6.5556221e-27 1.0000000e+00 2.1765903e-15]\n",
      "found  0 berry 7.174176e-18\n",
      "found  1 jam 8.900753e-27\n",
      "found  2 muffin 6.555622e-27\n",
      "found  3 popcorn 1.0\n",
      "found  4 raspberry 2.1765903e-15\n"
     ]
    }
   ],
   "source": [
    "img_path = \"../edge-server/popcorn.jpeg\"\n",
    "new_image = load_image(img_path)\n",
    "classify_image(new_image, classification_sess, classification_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./model/keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.cle ar_session()\n",
    "tfjs_target_dir = \"tfjs-models\"\n",
    "tfjs.converters.save_keras_model(model, tfjs_target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfjs.converters.convert_tf_saved_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
